{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../modules/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import set_random_seed\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "rescale_factor = 1.0\n",
    "random_seed = 42\n",
    "\n",
    "set_random_seed(random_seed)\n",
    "full_path = \"/tmp2/itsmystyle/h4_data/FullLengthVideos\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullLengthVideosDataset(Dataset):\n",
    "    def __init__(self, video_path, video_label_path, rescale_factor=1.0, length=100, overlap=15, test=False, sorting=False, transform=None):\n",
    "        self.video_path = video_path\n",
    "        self.video_label_path = video_label_path\n",
    "        self.rescale_factor = rescale_factor\n",
    "        self.length = length\n",
    "        self.overlap = overlap\n",
    "        self.test = test\n",
    "        self.sorting = sorting\n",
    "        \n",
    "        if transform != None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STD)])\n",
    "        \n",
    "        print('preparing data...')\n",
    "        self.categories = sorted(os.listdir(self.video_path))\n",
    "        self.datas = []\n",
    "        for category in tqdm(self.categories, total=len(self.categories)):\n",
    "            \n",
    "            ### frames\n",
    "            frames_path = sorted(glob.glob(os.path.join(self.video_path, category, '*')))\n",
    "            frames = []\n",
    "            for path in frames_path:\n",
    "#                 frame = skimage.io.imread(path)\n",
    "#                 frame = skimage.transform.rescale(frame, rescale_factor, mode='constant', preserve_range=True, multichannel=True, anti_aliasing=True).astype(np.uint8)\n",
    "#                 frame.append(frame)\n",
    "                frame = Image.open(path)\n",
    "                frames.append(np.array(frame).astype(np.uint8))\n",
    "            frames = np.stack(frames)\n",
    "            \n",
    "            ### labels\n",
    "            if not self.test:\n",
    "                labels_path = os.path.join(self.video_label_path, '{}.txt'.format(category))\n",
    "                with open(labels_path, 'r') as fin:\n",
    "                    labels = fin.readlines()\n",
    "                labels = np.array([int(i.strip()) for i in labels])\n",
    "            else:\n",
    "                labels = np.array([0] * len(frames_path))\n",
    "            \n",
    "            self.datas += self.trim_frames(frames, labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.datas[index]\n",
    "    \n",
    "    def collate_fn(self, datas):\n",
    "        batch = {}\n",
    "        \n",
    "        lens = [data[0].shape[0] for data in datas]\n",
    "        padding_len = max(lens)\n",
    "        \n",
    "        if self.sorting:\n",
    "            # sort whole datas with its video length\n",
    "            sorted_idx = np.argsort(lens)[::-1]\n",
    "            datas = np.array(datas)[sorted_idx]\n",
    "            \n",
    "        # frames_len\n",
    "        frames_len = [data[0].shape[0] for data in datas]\n",
    "        batch[\"frames_len\"] = frames_len\n",
    "        \n",
    "        # frames\n",
    "        batch_size = len(datas)\n",
    "        width, height, channel = datas[0][0].shape[1:]\n",
    "        frames = torch.zeros((batch_size, padding_len, channel, width, height))\n",
    "        frames[:,:,0,:,:] = (frames[:,:,0,:,:] - MEAN[0]) / STD[0]\n",
    "        frames[:,:,1,:,:] = (frames[:,:,1,:,:] - MEAN[1]) / STD[1]\n",
    "        frames[:,:,2,:,:] = (frames[:,:,2,:,:] - MEAN[2]) / STD[2]\n",
    "        for idx, (data, _) in enumerate(datas):\n",
    "            for step, frame in enumerate(data):\n",
    "                frames[idx, step] = self.transform(frame)\n",
    "        batch['frames'] = frames.float()\n",
    "        \n",
    "        if not self.test:\n",
    "            # labels\n",
    "            labels = np.zeros((batch_size, padding_len), dtype=np.int64)\n",
    "            for idx, (_, data) in enumerate(datas):\n",
    "                labels[idx, :data.shape[0]] = data\n",
    "            batch['labels'] = torch.tensor(labels).long()\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "    def trim_frames(self, frames, labels):\n",
    "        chunk_size = frames.shape[0] // (self.length - self.overlap)\n",
    "\n",
    "        frame_chunks = np.array_split(frames, chunk_size)\n",
    "        label_chunks = np.array_split(labels, chunk_size)\n",
    "\n",
    "        final_chunks = []\n",
    "\n",
    "        for i in range(chunk_size):\n",
    "            if self.overlap > 0:\n",
    "                if i == 0:\n",
    "                    final_chunks.append((frame_chunks[i], label_chunks[i]))\n",
    "                else:\n",
    "                    frame_chunk = np.concatenate((frame_chunks[i-1][-self.overlap:], frame_chunks[i]))\n",
    "                    label_chunk = np.concatenate((label_chunks[i-1][-self.overlap:], label_chunks[i]))\n",
    "                    final_chunks.append((frame_chunk, label_chunk))\n",
    "            else:\n",
    "                final_chunks.append((frame_chunks[i], label_chunks[i]))\n",
    "\n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7befc2bee9a64a75a78bc32e8eeca5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = FullLengthVideosDataset(os.path.join(full_path, 'videos', 'valid'), os.path.join(full_path, 'labels', 'valid'), length=80, overlap=20, sorting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, num_workers=0, shuffle=False, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e8916501140af8bea5a69154a4b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=37.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (83,240,320,3) into shape (83)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-411ccddd3bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/dlp36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/dlp36/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/envs/dlp36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-08bc48b32423>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(self, datas)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# sort whole datas with its video length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0msorted_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# frames_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (83,240,320,3) into shape (83)"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(dataloader, total=len(dataloader)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
