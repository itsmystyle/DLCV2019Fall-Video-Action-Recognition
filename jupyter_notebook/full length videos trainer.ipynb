{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../modules/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import set_random_seed\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "rescale_factor = 1.0\n",
    "random_seed = 42\n",
    "\n",
    "set_random_seed(random_seed)\n",
    "full_path = \"../h4_data/FullLengthVideos/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullLengthVideosDataset(Dataset):\n",
    "    def __init__(self, video_path, video_label_path, rescale_factor=1.0, length=100, overlap=15, test=False, sorting=False, transform=None):\n",
    "        self.video_path = video_path\n",
    "        self.video_label_path = video_label_path\n",
    "        self.rescale_factor = rescale_factor\n",
    "        self.length = length\n",
    "        self.overlap = overlap\n",
    "        self.test = test\n",
    "        self.sorting = sorting\n",
    "        \n",
    "        if transform != None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STD)])\n",
    "        \n",
    "        print('preparing data...')\n",
    "        self.categories = sorted(os.listdir(self.video_path))\n",
    "        self.datas = []\n",
    "        for category in tqdm(self.categories, total=len(self.categories)):\n",
    "            \n",
    "            ### frames\n",
    "            frames_path = sorted(glob.glob(os.path.join(self.video_path, category, '*')))\n",
    "            frames = []\n",
    "            for path in frames_path:\n",
    "#                 frame = skimage.io.imread(path)\n",
    "#                 frame = skimage.transform.rescale(frame, rescale_factor, mode='constant', preserve_range=True, multichannel=True, anti_aliasing=True).astype(np.uint8)\n",
    "#                 frame.append(frame)\n",
    "                frame = Image.open(path)\n",
    "                frames.append(np.array(frame).astype(np.uint8))\n",
    "            frames = np.stack(frames)\n",
    "            \n",
    "            ### labels\n",
    "            if not self.test:\n",
    "                labels_path = os.path.join(self.video_label_path, '{}.txt'.format(category))\n",
    "                with open(labels_path, 'r') as fin:\n",
    "                    labels = fin.readlines()\n",
    "                labels = np.array([int(i.strip()) for i in labels])\n",
    "            else:\n",
    "                labels = np.array([0] * len(frames_path))\n",
    "            \n",
    "            self.datas += self.trim_frames(frames, labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.datas[index]\n",
    "    \n",
    "    def collate_fn(self, datas):\n",
    "        batch = {}\n",
    "        \n",
    "        lens = [data[0].shape[0] for data in datas]\n",
    "        padding_len = max(lens)\n",
    "        \n",
    "        if self.sorting:\n",
    "            # sort whole datas with its video length\n",
    "            sorted_idx = np.argsort(lens)[::-1]\n",
    "            datas = [datas[idx] for idx in sorted_idx]\n",
    "            \n",
    "        # frames_len\n",
    "        frames_len = [data[0].shape[0] for data in datas]\n",
    "        batch[\"frames_len\"] = frames_len\n",
    "        \n",
    "        # frames\n",
    "        batch_size = len(datas)\n",
    "        width, height, channel = datas[0][0].shape[1:]\n",
    "        frames = torch.zeros((batch_size, padding_len, channel, width, height))\n",
    "        frames[:,:,0,:,:] = (frames[:,:,0,:,:] - MEAN[0]) / STD[0]\n",
    "        frames[:,:,1,:,:] = (frames[:,:,1,:,:] - MEAN[1]) / STD[1]\n",
    "        frames[:,:,2,:,:] = (frames[:,:,2,:,:] - MEAN[2]) / STD[2]\n",
    "        for idx, (data, _) in enumerate(datas):\n",
    "            for step, frame in enumerate(data):\n",
    "                frames[idx, step] = self.transform(frame)\n",
    "        batch['frames'] = frames.float()\n",
    "        \n",
    "        if not self.test:\n",
    "            # labels\n",
    "            labels = np.zeros((batch_size, padding_len), dtype=np.int64)\n",
    "            for idx, (_, data) in enumerate(datas):\n",
    "                labels[idx, :data.shape[0]] = data\n",
    "            batch['labels'] = torch.tensor(labels).long()\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "    def trim_frames(self, frames, labels):\n",
    "        chunk_size = frames.shape[0] // (self.length - self.overlap)\n",
    "\n",
    "        frame_chunks = np.array_split(frames, chunk_size)\n",
    "        label_chunks = np.array_split(labels, chunk_size)\n",
    "\n",
    "        final_chunks = []\n",
    "\n",
    "        for i in range(chunk_size):\n",
    "            if self.overlap > 0:\n",
    "                if i == 0:\n",
    "                    final_chunks.append((frame_chunks[i], label_chunks[i]))\n",
    "                else:\n",
    "                    frame_chunk = np.concatenate((frame_chunks[i-1][-self.overlap:], frame_chunks[i]))\n",
    "                    label_chunk = np.concatenate((label_chunks[i-1][-self.overlap:], label_chunks[i]))\n",
    "                    final_chunks.append((frame_chunk, label_chunk))\n",
    "            else:\n",
    "                final_chunks.append((frame_chunks[i], label_chunks[i]))\n",
    "\n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5f82d01fb84d0983d5f48754d7e311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = FullLengthVideosDataset(os.path.join(full_path, 'videos', 'valid'), os.path.join(full_path, 'labels', 'valid'), length=80, overlap=20, sorting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, num_workers=32, shuffle=False, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9aed8c48484cad9a196e42b292fdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=37.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(dataloader, total=len(dataloader)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
